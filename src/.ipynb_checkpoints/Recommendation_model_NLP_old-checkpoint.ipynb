{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stackoverflow tag recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import gensim\n",
    "from model import ConvText\n",
    "\n",
    "random_state = 747"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Data\n",
    "\n",
    "Stackoverflow question data downloaded from Google BigQuery and aggregated into a single csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../../../Data/stackoverflow/questions.csv'\n",
    "df = pd.read_csv(file, usecols=['title', 'tags'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is too big for taining. We will only consider randomly selected 100,000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = shuffle(df, random_state=random_state)\n",
    "df = df[:1000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean title text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9#+-]\", \" \", text.lower())\n",
    "    return text\n",
    "\n",
    "df['title'] = df['title'].apply(clean_text)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../../Data/stackoverflow/clean_questions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../Data/stackoverflow/clean_questions.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data as list and basic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df['title'].tolist()\n",
    "tags = df['tags'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the number of words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The total number of words in the data is: ', sum([len(text.split()) for text in questions]))\n",
    "\n",
    "def tokenize_question(text):\n",
    "    return text.split()\n",
    "\n",
    "question_vect = CountVectorizer(tokenizer=tokenize_question)\n",
    "question_vect.fit(questions)\n",
    "\n",
    "print('The number of words in the vocabulary is: ', len(question_vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the number of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tags(text):\n",
    "    return text.split('|')\n",
    "\n",
    "tags_vect = CountVectorizer(tokenizer=tokenize_tags)\n",
    "tags_vect.fit(tags)\n",
    "\n",
    "print('The total number of tags is: ', len(tags_vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save tag label as vectorized tokens.\n",
    "\n",
    "There are too many tags to predict. In our model we will only look at the top 100 tags and save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tags = 100\n",
    "\n",
    "def tokenize_tags(text):\n",
    "    return text.split('|')\n",
    "\n",
    "tags_vect = CountVectorizer(tokenizer=tokenize_tags, max_features=max_tags)\n",
    "tags = tags_vect.fit_transform(tags)\n",
    "tags = tags.toarray()\n",
    "print('Number of tags: ', len(tags_vect.vocabulary_))\n",
    "\n",
    "tags_token = tags_vect.get_feature_names()\n",
    "tag_frequency = tags.sum(axis=0)\n",
    "print('The list of tags with frequency is: ')\n",
    "print(dict(zip(tags_token, tag_frequency)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of number of tags in each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tags.sum(axis=1))\n",
    "plt.xlabel('Number of tags for a question')\n",
    "plt.ylabel('Number of questions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split test and train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(questions, tags, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample weight\n",
    "Evaluate sample weight from y_train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight = compute_sample_weight('balanced', y_test)\n",
    "sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(y_test, y_predicted, print_metrics=True):\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    precision = precision_score(y_test, y_predicted, average='weighted')\n",
    "    recall = recall_score(y_test, y_predicted, average='weighted')\n",
    "    f1 = f1_score(y_test, y_predicted, average='weighted')\n",
    "    \n",
    "    if print_metrics:\n",
    "        print(\"accuracy: %.3f - precision: %.3f - recall: %.3f - f1: %.3f\" % (\n",
    "            accuracy, precision, recall, f1))\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple first order model: Bag of words with logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words embedding for quesitons\n",
    "\n",
    "Remove common words and words appearing very less number of times from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_question(text):\n",
    "    return text.split()\n",
    "\n",
    "bag_vect = CountVectorizer(tokenizer=tokenize_question,\n",
    "                               stop_words='english',\n",
    "                               min_df=3,\n",
    "                               max_df=0.5)\n",
    "\n",
    "X_train_bag = bag_vect.fit_transform(X_train)\n",
    "X_test_bag = bag_vect.transform(X_test)\n",
    "print('The number of words in the vocabulary is: ', len(bag_vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with binary relevance\n",
    "Since this is a multi-label classificaiton, we will use binary relevance on top of logistical regression. This basically splits each label as a seperate classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bag_log_clf = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "bag_log_clf.fit(X_train_bag, y_train)\n",
    "\n",
    "print('Train score')\n",
    "y_train_bag_predict = bag_log_clf.predict(X_train_bag)\n",
    "eval_metrics(y_train, y_train_bag_predict)\n",
    "\n",
    "print('Test score')\n",
    "y_test_bag_predict = bag_log_clf.predict(X_test_bag)\n",
    "eval_metrics(y_test, y_test_bag_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = importances = bag_log_clf.estimators_[0].coef_[0]\n",
    "for i in range(1,len(bag_log_clf.estimators_)):\n",
    "    importances += bag_log_clf.estimators_[i].coef_[0]\n",
    "\n",
    "importances = importances/sum(importances)\n",
    "    \n",
    "feature_imps = {'importances':importances, 'feature':bag_vect.get_feature_names()}\n",
    "feature_imps = pd.DataFrame(feature_imps)\n",
    "feature_imps = feature_imps.sort_values('importances', ascending=False)\n",
    "# Normalize importance and add cumulative importance\n",
    "feature_imps['cum_imp'] = feature_imps['importances'].cumsum()\n",
    "feature_imps['importances'] = feature_imps['importances']/feature_imps['importances'].max()\n",
    "feature_imps = feature_imps.reset_index(drop=True)\n",
    "feature_imps['no_features'] = feature_imps.index + 1\n",
    "feature_imps[['feature', 'importances', 'cum_imp']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF with logistic regression and binary relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF on bag of words embedding for quesitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_question(text):\n",
    "    return text.split()\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=tokenize_question,\n",
    "                               stop_words='english',\n",
    "                               min_df=4,\n",
    "                               max_df=0.5)\n",
    "\n",
    "X_train_tfidf = tfidf_vect.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)\n",
    "print('The number of words in the vocabulary is: ', len(tfidf_vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with binary relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tfidf_log_clf = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "tfidf_log_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Train score')\n",
    "y_train_tfidf_predict = tfidf_log_clf.predict(X_train_tfidf)\n",
    "eval_metrics(y_train, y_train_tfidf_predict)\n",
    "\n",
    "print('Test score')\n",
    "y_test_tfidf_predict = tfidf_log_clf.predict(X_test_tfidf)\n",
    "eval_metrics(y_test, y_test_tfidf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = importances = tfidf_log_clf.estimators_[0].coef_[0]\n",
    "for i in range(1,len(tfidf_log_clf.estimators_)):\n",
    "    importances += tfidf_log_clf.estimators_[i].coef_[0]\n",
    "\n",
    "importances = importances/sum(importances)\n",
    "    \n",
    "feature_imps = {'importances':importances, 'feature':tfidf_vect.get_feature_names()}\n",
    "feature_imps = pd.DataFrame(feature_imps)\n",
    "feature_imps = feature_imps.sort_values('importances', ascending=False)\n",
    "# Normalize importance and add cumulative importance\n",
    "feature_imps['cum_imp'] = feature_imps['importances'].cumsum()\n",
    "feature_imps['importances'] = feature_imps['importances']/feature_imps['importances'].max()\n",
    "feature_imps = feature_imps.reset_index(drop=True)\n",
    "feature_imps['no_features'] = feature_imps.index + 1\n",
    "feature_imps[['feature', 'importances', 'cum_imp']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF with Naives Bayes Classifier and binary relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF on bag of words embedding for quesitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_question(text):\n",
    "    return text.split()\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=tokenize_question,\n",
    "                               stop_words='english',\n",
    "                               min_df=4,\n",
    "                               max_df=0.5)\n",
    "\n",
    "X_train_tfidf = tfidf_vect.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)\n",
    "print('The number of words in the vocabulary is: ', len(tfidf_vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naives Bayes with binary relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tfidf_log_clf = OneVsRestClassifier(GaussianNB())\n",
    "\n",
    "tfidf_log_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('Train score')\n",
    "y_train_tfidf_predict = tfidf_log_clf.predict(X_train_tfidf)\n",
    "eval_metrics(y_train, y_train_tfidf_predict)\n",
    "\n",
    "print('Test score')\n",
    "y_test_tfidf_predict = tfidf_log_clf.predict(X_test_tfidf)\n",
    "eval_metrics(y_test, y_test_tfidf_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = importances = tfidf_log_clf.estimators_[0].coef_[0]\n",
    "for i in range(1,len(tfidf_log_clf.estimators_)):\n",
    "    importances += tfidf_log_clf.estimators_[i].coef_[0]\n",
    "\n",
    "importances = importances/sum(importances)\n",
    "    \n",
    "feature_imps = {'importances':importances, 'feature':tfidf_vect.get_feature_names()}\n",
    "feature_imps = pd.DataFrame(feature_imps)\n",
    "feature_imps = feature_imps.sort_values('importances', ascending=False)\n",
    "# Normalize importance and add cumulative importance\n",
    "feature_imps['cum_imp'] = feature_imps['importances'].cumsum()\n",
    "feature_imps['importances'] = feature_imps['importances']/feature_imps['importances'].max()\n",
    "feature_imps = feature_imps.reset_index(drop=True)\n",
    "feature_imps['no_features'] = feature_imps.index + 1\n",
    "feature_imps[['feature', 'importances', 'cum_imp']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional sentence classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "word2vec_path = '../../../Data/stackoverflow/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([len(x) for x in X_train], bins=50)\n",
    "plt.xlabel('Length of sequence')\n",
    "plt.ylabel('number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyerparameters related to sentece creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 35\n",
    "vocab_size = 100000\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=True, split=' ', filters='')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "tokenizer.num_words = vocab_size\n",
    "\n",
    "seq_train = tokenizer.texts_to_sequences(X_train)\n",
    "seq_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "seq_length = [len(x) for x in seq_train]\n",
    "max_seq_length = max(seq_length)\n",
    "\n",
    "seq_train = pad_sequences(seq_train, maxlen=max_seq_length, padding='post', truncating='post')\n",
    "seq_test = pad_sequences(seq_test, maxlen=max_seq_length, padding='post', truncating='post')\n",
    "\n",
    "embedding_weights = np.zeros((vocab_size+1, embedding_size))\n",
    "word2vec_counter, randword_counter = 0, 0\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocab_size:\n",
    "        break\n",
    "        \n",
    "    if word in word2vec:\n",
    "        embedding_weights[index,:] = word2vec[word]\n",
    "        word2vec_counter += 1\n",
    "    else:\n",
    "        embedding_weights[index,:] = np.random.normal(0, 0.15, embedding_size)\n",
    "        randword_counter += 1\n",
    "        \n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(seq_length, bins=50)\n",
    "plt.xlabel('Length of sequence')\n",
    "plt.ylabel('number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - simplified convolutional neural network\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Question_Sequence (InputLayer)  (None, 35)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding_lookup (Embedding)    (None, 35, 300)      10500300    Question_Sequence[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer1_filtsz_3_stride_1 (Conv1 (None, 33, 32)       28832       Embedding_lookup[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer1_filtsz_4_stride_1 (Conv1 (None, 32, 32)       38432       Embedding_lookup[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer1_filtsz_5_stride_1 (Conv1 (None, 31, 32)       48032       Embedding_lookup[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer1_merge (Concatenate)      (None, 96, 32)       0           layer1_filtsz_3_stride_1[0][0]   \n",
      "                                                                 layer1_filtsz_4_stride_1[0][0]   \n",
      "                                                                 layer1_filtsz_5_stride_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer2_filtsz_5_stride_2 (Conv1 (None, 46, 32)       5152        layer1_merge[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer3 (Flatten)                (None, 1472)         0           layer2_filtsz_5_stride_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer3_dense (Dense)            (None, 250)          368250      layer3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "logit_dense_sigmoid (Dense)     (None, 100)          25100       layer3_dense[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 11,014,098\n",
      "Trainable params: 11,014,098\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_weights = np.zeros((35000+1, 300))\n",
    "\n",
    "max_seq_length=35\n",
    "embedding_layer = Embedding(embedding_weights.shape[0],\n",
    "                            embedding_weights.shape[1],\n",
    "                            weights=[embedding_weights],\n",
    "                            input_length=max_seq_length,\n",
    "                            trainable=True,\n",
    "                            name='Embedding_lookup')\n",
    "\n",
    "sequence_input = Input(shape=(max_seq_length,), dtype='int32', name='Question_Sequence')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "convs = []\n",
    "filter_sizes = [3,4,5]\n",
    "\n",
    "l_conv = Conv1D(32,3,activation='relu',strides=1,name='layer1_filtsz_3_stride_1')(embedded_sequences)\n",
    "convs.append(l_conv)\n",
    "l_conv = Conv1D(32,4,activation='relu',strides=1,name='layer1_filtsz_4_stride_1')(embedded_sequences)\n",
    "convs.append(l_conv)\n",
    "l_conv = Conv1D(32,5,activation='relu',strides=1,name='layer1_filtsz_5_stride_1')(embedded_sequences)\n",
    "convs.append(l_conv)\n",
    "\n",
    "\n",
    "l_merge = concatenate(convs, axis=1, name='layer1_merge')\n",
    "\n",
    "l_cov1= Conv1D(32, 5, activation='relu',strides=2, name='layer2_filtsz_5_stride_2')(l_merge)\n",
    "# l_pool1 = MaxPooling1D(3)(l_cov1)\n",
    "\n",
    "\n",
    "l_flat = Flatten(name='layer3')(l_cov1)\n",
    "l_dense = Dense(250, activation='relu',name='layer3_dense')(l_flat)\n",
    "preds = Dense(100, activation='softmax',name='logit_dense_sigmoid')(l_dense)\n",
    "\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - simplified convolutional neural network\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file='model.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(tags)\n",
    "max_seq_length = 35\n",
    "vocab_size = 100000\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ConvText\n",
    "conv_model = ConvText(max_seq_length, max_tags, embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model.fit(x=seq_train[:200000,:], y=y_train[:200000,:], batch_size=100, val_x=seq_test, val_y=y_test, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model2 import ConvText\n",
    "conv_model = ConvText(max_seq_length, max_tags, embedding_weights)\n",
    "conv_model.fit(x=seq_train[:200000,:], y=y_train[:200000,:], batch_size=100, val_x=seq_test, val_y=y_test, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
